{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 id=\"xgboost-classifier-algorithm\" align=\"center\">üì¶ XGBoost Classifier üì¶</h1>\n\n<center><i>Combining Different Models to get Accurate Classifications<i></center>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"----\n\n<h1 id='brief-description'>üìù Brief Description</h1>\n\n`XGBoost` Algorithm consists in a group of `different Machine Learning Ensemble Models` in order to get accurate results.\n\n<center>\n<img src='https://media.geeksforgeeks.org/wp-content/uploads/20210707140912/Bagging.png' />\n</center>\n\n<br />\n\n**‚úîÔ∏è Pros:**\n\n```\n- Gradient Boosting comes with an easy to read and interpret algorithm, making most of its predictions easy to handle;\n\n- Boosting is a resilient and robust method that prevents and cubs over-fitting quite easily;\n\n- XGBoost performs very well on medium, small, data with subgroups and structured datasets with not too many features;\n\n- Less feature engineering required (no need for scaling, normalizing data, can also handle missing values well);\n\n- Feature importance can be found out (it output importance of each feature, can be used for feature selection)\n\n- Handles large sized datasets well;\n\n- good execution speed;\n\n- good model performance (wins most of the Kaggle competitions);\n\n- less prone to overfitting.\n```\n\n<br />\n\n**‚ùå Cons:**\n\n```\n- XGBoost does not perform so well on sparse and unstructured data;\n\n- the Algorithm is very sensitive to outliers since every classifier is forced to fix the errors in the predecessor learners;\n\n- The overall method is hardly scalable. This is because the estimators base their correctness on previous predictors, hence the procedure involves a lot of struggle to streamline;\n\n- difficult interpretation, visualization tough;\n\n- overfitting possible if parameters not tuned properly;\n\n- harder to tune as there are too many hyperparameters.\n```\n\n<br />\n\n**üìõ Some XGB Classifier Properties:**\n\n```\n- objective: evaluation method\n- n_estimators: number of ensembles\n- learning_rate: the minimum value to identify whether the model is learning or not\n- colsample_by_tree: features' percentage used by the ensemble\n- max_depth: max depth for each ensemble\n- n_jobs: number of processors used over the trianing and prediction steps\n\n- early_stopping_rounds: number of subsequent epochs/rounds the model is not improving the learning rate\n- eval_set: dataset for evaluation (it is commonly used the validation one)\n- verbose: whether the training log will be or will not be registered to thee user on the screen\n```","metadata":{}},{"cell_type":"markdown","source":"----\n\n<h1 id='reach-me'>‚ÑπÔ∏è Further Information</h1>\n<br/>\n\nFor further information, check out these four videos from *[StatQuest with Josh Starmer](https://www.youtube.com/@statquest)* YouTube channel:\n\n- *[XGBoost Part 1 (of 4): Regression](https://www.youtube.com/watch?v=OtD8wVaFm6E)*\n- *[XGBoost Part 2 (of 4): Classification](https://www.youtube.com/watch?v=8b1JEDvenQU)*\n- *[XGBoost Part 3 (of 4): Mathematical Details](https://www.youtube.com/watch?v=ZVFeW798-2I)*\n- *[XGBoost Part 4 (of 4): Crazy Cool Optimizations](https://www.youtube.com/watch?v=oRrKeUCEbq8)*","metadata":{}},{"cell_type":"markdown","source":"----\n\n<h1 id='example-code'>üíª Example Code</h1>\n<br/>\n\nLet's use `XGBoost` package to demonstrate how to create, fit, make predictions and evaluate a simple `XGBoost Classifier Model`.\n\nTo evaluation, we will be using the `Classification Accuracy` as the metric to evaluate the results.\n\nIn Binary Classification Problems, our models can have four predictions:\n\n<br />\n\n> **True Positive (TP)** - the model predicted `true`, and the real outcome is `true`; ‚úîÔ∏è\n\n> **True Negative (TN)** - the model predicted `false`, and the real outcome is `false`; ‚úîÔ∏è\n\n> **False Positive (FP)** - the model predicted `true`, and the real outcome is `false`; ‚ùå\n\n> **False Negative (FN)** - the model predicted `false`, and the real outcome is `true`; ‚ùå\n\n<br />\n\nWith this in mind, Classification Accuracy is calculated adding the True Positives and True Negatives, anbd dividing the result by the sum between True Positives, True Negatives, False Positives and False Negatives. Which means: \n\n$(TP + TN) / (TP + TN + FP + FN)$\n\nSo, consider that the real dataset has `150 True outcomes` and `150 False outcomes` and that my model has predicted `100 True Positives`, `100 True Negatives`, `50 False Positives` and `50 False Negatives`, the model's accuracy will be:\n\n$(TP + TN) / (TP + TN + FP + FN)$\n\n$(100 + 100) / (100 + 100 + 50 + 50)$\n\n$200 / 300 == 2 / 3 =~ 0.67 == 67$%\n\n<br />\n<br />\n\nNow, let's hop into the code!!","metadata":{}},{"cell_type":"code","source":"# Setting up the environment\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nimport math\n\nSEED = (2000)\nX_MIN = (0)\nX_MAX = (100)\nY_MIN = (0)\nY_MAX = (2)\n\nTRAIN_SAMPLES = (800)\nVALID_SAMPLES = (20)\n\nnp.random.seed(SEED)\npd.set_option('display.max_rows', 15)\npd.set_option('display.max_columns', 15)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T00:40:07.304041Z","iopub.execute_input":"2022-12-20T00:40:07.304407Z","iopub.status.idle":"2022-12-20T00:40:07.310554Z","shell.execute_reply.started":"2022-12-20T00:40:07.304376Z","shell.execute_reply":"2022-12-20T00:40:07.309746Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Generating fake dataset\nX_train = np.random.randint(X_MIN, X_MAX, TRAIN_SAMPLES)\nX_valid = np.random.randint(X_MIN, X_MAX, VALID_SAMPLES)\ny_train = np.random.randint(Y_MIN, Y_MAX, TRAIN_SAMPLES)\ny_valid = np.random.randint(Y_MIN, Y_MAX, VALID_SAMPLES)\n\nX_train = pd.DataFrame(X_train, columns=['X'])\nX_valid = pd.DataFrame(X_valid, columns=['X'])\ny_train = pd.DataFrame(y_train, columns=['y'])\ny_valid = pd.DataFrame(y_valid, columns=['y'])","metadata":{"execution":{"iopub.status.busy":"2022-12-20T00:38:36.733117Z","iopub.execute_input":"2022-12-20T00:38:36.733488Z","iopub.status.idle":"2022-12-20T00:38:36.741894Z","shell.execute_reply.started":"2022-12-20T00:38:36.733457Z","shell.execute_reply":"2022-12-20T00:38:36.740882Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Creating the model\nxgb_model = xgb.XGBClassifier(\n    objective='binary:logistic'\n    , n_estimators=250\n    , learning_rate=0.10\n    , colsample_bytree=0.70\n    , max_depth=3\n    , n_jobs=4\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T00:38:44.187505Z","iopub.execute_input":"2022-12-20T00:38:44.187871Z","iopub.status.idle":"2022-12-20T00:38:44.193297Z","shell.execute_reply.started":"2022-12-20T00:38:44.187842Z","shell.execute_reply":"2022-12-20T00:38:44.192162Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Training and making predictions\nxgb_model.fit(\n    X_train, y_train\n    , early_stopping_rounds=5\n    , eval_set=[(X_valid, y_valid)]\n    , verbose=False\n)\n\nprint('Training Done!')\n\npredictions = xgb_model.predict(X_valid)\n\nprint('Predictions Done!')","metadata":{"execution":{"iopub.status.busy":"2022-12-20T00:38:56.874009Z","iopub.execute_input":"2022-12-20T00:38:56.874386Z","iopub.status.idle":"2022-12-20T00:38:56.921437Z","shell.execute_reply.started":"2022-12-20T00:38:56.874356Z","shell.execute_reply":"2022-12-20T00:38:56.920324Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Training Done!\nPredictions Done!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Accuracy\naccuracy = accuracy_score(y_valid, predictions)\ntrain_score = round(xgb_model.score(X_train, y_train) * 100, 2)\nvalid_score = round(xgb_model.score(X_valid, y_valid) * 100, 2)\n\nprint('Accuracy: ', round(accuracy * 100, 2), '%', sep='')","metadata":{"execution":{"iopub.status.busy":"2022-12-20T00:41:23.666453Z","iopub.execute_input":"2022-12-20T00:41:23.666851Z","iopub.status.idle":"2022-12-20T00:41:23.690695Z","shell.execute_reply.started":"2022-12-20T00:41:23.666798Z","shell.execute_reply":"2022-12-20T00:41:23.689611Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Accuracy: 40.0%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**OBS.:** *as far as the goal of this Kernel is to explain what is and how to apply `XGB Classifier Algorithm`, we have not done any Data Preprocessing and Transformation, so our model's evaluation is quite suck! Do not worry it üòÇ*","metadata":{}},{"cell_type":"markdown","source":"----\n\nThank so much for today, see ya!! üëãüëã\n\n<br/>\n<h1 id='reach-me'>üì´ Reach Me</h1>\n<br/>\n\n> **Email:** **[csfelix08@gmail.com](mailto:csfelix08@gmail.com?)**\n\n> **Linkedin:** **[linkedin.com/in/csfelix/](https://www.linkedin.com/in/csfelix/)**\n\n> **Instagram:** **[instagram.com/c0deplus/](https://www.instagram.com/c0deplus/)**\n\n> **Portfolio:** **[CSFelix.io](https://csfelix.github.io/)**\n\n> **Kaggle:** **[DSFelix](https://www.kaggle.com/dsfelix)**","metadata":{}}]}
