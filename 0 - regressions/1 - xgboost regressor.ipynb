{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 id=\"random-forest-regressor-algorithm\" align=\"center\">üìà XGBoost Regressor üìà</h1>\n\n<center><i>Combining Different Models to get Accurate Predictions<i></center>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"----\n\n<h1 id='brief-description'>üìù Brief Description</h1>\n\n`XGBoost` Algorithm consists in a group of `different Machine Learning Ensemble Models` in order to get accurate results.\n\n<center>\n<img src='https://media.geeksforgeeks.org/wp-content/uploads/20210707140912/Bagging.png' />\n</center>\n\n<br />\n\n**‚úîÔ∏è Pros:**\n\n```\n- Gradient Boosting comes with an easy to read and interpret algorithm, making most of its predictions easy to handle;\n\n- Boosting is a resilient and robust method that prevents and cubs over-fitting quite easily;\n\n- XGBoost performs very well on medium, small, data with subgroups and structured datasets with not too many features;\n\n- Less feature engineering required (no need for scaling, normalizing data, can also handle missing values well);\n\n- Feature importance can be found out (it output importance of each feature, can be used for feature selection)\n\n- Handles large sized datasets well;\n\n- good execution speed;\n\n- good model performance (wins most of the Kaggle competitions);\n\n- less prone to overfitting.\n```\n\n<br />\n\n**‚ùå Cons:**\n\n```\n- XGBoost does not perform so well on sparse and unstructured data;\n\n- the Algorithm is very sensitive to outliers since every classifier is forced to fix the errors in the predecessor learners;\n\n- The overall method is hardly scalable. This is because the estimators base their correctness on previous predictors, hence the procedure involves a lot of struggle to streamline;\n\n- difficult interpretation, visualization tough;\n\n- overfitting possible if parameters not tuned properly;\n\n- harder to tune as there are too many hyperparameters.\n```\n\n<br />\n\n**üìõ Some XGB Regressor Properties:**\n\n```\n- objective: evaluation method\n- n_estimators: number of ensembles\n- learning_rate: the minimum value to identify whether the model is learning or not\n- colsample_by_tree: features' percentage used by the ensemble\n- max_depth: max depth for each ensemble\n- n_jobs: number of processors used over the trianing and prediction steps\n\n- early_stopping_rounds: number of subsequent epochs/rounds the model is not improving the learning rate\n- eval_set: dataset for evaluation (it is commonly used the validation one)\n- verbose: whether the training log will be or will not be registered to thee user on the screen\n```","metadata":{}},{"cell_type":"markdown","source":"----\n\n<h1 id='reach-me'>‚ÑπÔ∏è Further Information</h1>\n<br/>\n\nFor further information, check out these four videos from *[StatQuest with Josh Starmer](https://www.youtube.com/@statquest)* YouTube channel:\n\n- *[XGBoost Part 1 (of 4): Regression](https://www.youtube.com/watch?v=OtD8wVaFm6E)*\n- *[XGBoost Part 2 (of 4): Classification](https://www.youtube.com/watch?v=8b1JEDvenQU)*\n- *[XGBoost Part 3 (of 4): Mathematical Details](https://www.youtube.com/watch?v=ZVFeW798-2I)*\n- *[XGBoost Part 4 (of 4): Crazy Cool Optimizations](https://www.youtube.com/watch?v=oRrKeUCEbq8)*","metadata":{}},{"cell_type":"markdown","source":"----\n\n<h1 id='example-code'>üíª Example Code</h1>\n<br/>\n\nLet's use `XGBoost` package to demonstrate how to create, fit, make predictions and evaluate a simple `XGBoost Regressor Model`.\n\nTo evaluation, we will be using the `Root Mean Squared Error (RMSE)` Algorithm. This Algorithm works getting the absolute value of the substraction between the predicted values by the real ones. After that, we calculate the summatory between them, find out their mean and then calculate the square root of the result. The method can be repreented by the following equation:\n\n$sqrt(mean(sum(abs(predictedvalues - realvalues))))$\n\n\nNow, let's hop into the code!!","metadata":{}},{"cell_type":"code","source":"# Setting up the environment\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nimport math\n\nSEED = (2000)\nX_MIN = (0)\nX_MAX = (100)\nY_MIN = (0)\nY_MAX = (20)\n\nTRAIN_SAMPLES = (800)\nVALID_SAMPLES = (2)\n\nnp.random.seed(SEED)\npd.set_option('display.max_rows', 15)\npd.set_option('display.max_columns', 15)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T00:11:28.254807Z","iopub.execute_input":"2022-12-20T00:11:28.256179Z","iopub.status.idle":"2022-12-20T00:11:28.264985Z","shell.execute_reply.started":"2022-12-20T00:11:28.256126Z","shell.execute_reply":"2022-12-20T00:11:28.264020Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Generating fake dataset\nX_train = np.random.randint(X_MIN, X_MAX, TRAIN_SAMPLES)\nX_valid = np.random.randint(X_MIN, X_MAX, VALID_SAMPLES)\ny_train = np.random.randint(Y_MIN, Y_MAX, TRAIN_SAMPLES)\ny_valid = np.random.randint(Y_MIN, Y_MAX, VALID_SAMPLES)\n\nX_train = pd.DataFrame(X_train, columns=['X'])\nX_valid = pd.DataFrame(X_valid, columns=['X'])\ny_train = pd.DataFrame(y_train, columns=['y'])\ny_valid = pd.DataFrame(y_valid, columns=['y'])","metadata":{"execution":{"iopub.status.busy":"2022-12-20T00:04:54.648807Z","iopub.execute_input":"2022-12-20T00:04:54.649313Z","iopub.status.idle":"2022-12-20T00:04:54.665759Z","shell.execute_reply.started":"2022-12-20T00:04:54.649276Z","shell.execute_reply":"2022-12-20T00:04:54.663273Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Creating the model\nxgb_model = xgb.XGBRegressor(\n    objective='reg:squarederror'\n    , n_estimators=250\n    , learning_rate=0.10\n    , colsample_bytree=0.70\n    , max_depth=3\n    , n_jobs=4\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T00:06:52.719971Z","iopub.execute_input":"2022-12-20T00:06:52.720558Z","iopub.status.idle":"2022-12-20T00:06:52.729172Z","shell.execute_reply.started":"2022-12-20T00:06:52.720501Z","shell.execute_reply":"2022-12-20T00:06:52.727218Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Training and making predictions\nxgb_model.fit(\n    X_train, y_train\n    , early_stopping_rounds=5\n    , eval_set=[(X_valid, y_valid)]\n    , verbose=False\n)\n\nprint('Training Done!')\n\npredictions = xgb_model.predict(X_valid)\n\nprint('Predictions Done!')","metadata":{"execution":{"iopub.status.busy":"2022-12-20T00:21:40.361407Z","iopub.execute_input":"2022-12-20T00:21:40.361875Z","iopub.status.idle":"2022-12-20T00:21:40.698144Z","shell.execute_reply.started":"2022-12-20T00:21:40.361838Z","shell.execute_reply":"2022-12-20T00:21:40.696738Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:797: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  UserWarning,\n","output_type":"stream"},{"name":"stdout","text":"Training Done!\nPredictions Done!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluation\nrmse = math.sqrt(mean_squared_error(y_valid, predictions))\ntrain_score = round(xgb_model.score(X_train, y_train) * 100, 2)\nvalid_score = round(xgb_model.score(X_valid, y_valid) * 100, 2)\n\nprint('Root Mean Squared Error (RMSE):', rmse)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T00:11:33.150948Z","iopub.execute_input":"2022-12-20T00:11:33.152172Z","iopub.status.idle":"2022-12-20T00:11:33.182477Z","shell.execute_reply.started":"2022-12-20T00:11:33.152129Z","shell.execute_reply":"2022-12-20T00:11:33.181370Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Root Mean Squared Error (RMSE): 7.5749505298437265\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**OBS.:** *as far as the goal of this Kernel is to explain what is and how to apply `XGB Regressor Algorithm`, we have not done any Data Preprocessing and Transformation, so our model's evaluation is quite suck! Do not worry it üòÇ*","metadata":{}},{"cell_type":"markdown","source":"----\n\nThank so much for today, see ya!! üëãüëã\n\n<br/>\n<h1 id='reach-me'>üì´ Reach Me</h1>\n<br/>\n\n> **Email:** **[csfelix08@gmail.com](mailto:csfelix08@gmail.com?)**\n\n> **Linkedin:** **[linkedin.com/in/csfelix/](https://www.linkedin.com/in/csfelix/)**\n\n> **Instagram:** **[instagram.com/c0deplus/](https://www.instagram.com/c0deplus/)**\n\n> **Portfolio:** **[CSFelix.io](https://csfelix.github.io/)**\n\n> **Kaggle:** **[DSFelix](https://www.kaggle.com/dsfelix)**","metadata":{}}]}